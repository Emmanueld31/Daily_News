name: Daily News Routine

on:
  workflow_dispatch:
  schedule:
    - cron: '0 4 * * *'  # 03:00 UTC (~05:00 CEST / 04:00 CET)

concurrency:
  group: daily-news
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m playwright install --with-deps

      - name: Generate PDFs from RSS
        env:
          FEEDS_FILE: Feeds.txt
          OUTPUT_DIR: ${{ github.workspace }}/output
        run: |
          set -euo pipefail
          mkdir -p "$OUTPUT_DIR" logs
          # tee this step's output to logs only
          exec > >(tee -a logs/main.log) 2>&1

          echo "== Normalize feeds =="
          sed -i 's/\r$//' "$FEEDS_FILE" || true

          echo "== Start generation =="
          while IFS= read -r raw || [ -n "$raw" ]; do
            url="${raw%%#*}"; url="$(printf '%s' "$url" | xargs)"
            [[ -n "$url" && "$url" =~ ^https?:// ]] || continue
            echo "Processing Feed"
            python rss2pdf.py "$url" "$OUTPUT_DIR" || echo "::warning::Failed feed"
          done < "$FEEDS_FILE"

          echo "Generated files:"
          ls -lh "$OUTPUT_DIR" || true

      - name: Merge PDFs into single file (chunked max 1000 pages)
        env:
          OUTPUT_DIR: ${{ github.workspace }}/output
        run: |
          set -euo pipefail
          mkdir -p logs
          exec > >(tee -a logs/main.log) 2>&1

          DATE_SHORT=$(TZ=Europe/Amsterdam date +%y%m%d)
          BASE="${DATE_SHORT} - Daily Newspapers"
          OUT_BASE="$OUTPUT_DIR/$BASE"   # no .pdf; script emits “ - Part N.pdf”

          echo "== Merge with chunking =="
          python merge_pdfs.py --pattern "*.pdf" --skip-encrypted --max-pages 1000 -o "$OUT_BASE" "$OUTPUT_DIR"

          echo "Merged parts:"
          # robust listing (handles spaces/dashes safely)
          find "$OUTPUT_DIR" -maxdepth 1 -type f -name "${BASE} - Part *.pdf" -print0 | xargs -0 ls -lh || true

      - name: Package output (all merged parts) and logs
        if: always()
        run: |
          set -euo pipefail
          DATE_SHORT=$(TZ=Europe/Amsterdam date +%y%m%d)
          BASE="${DATE_SHORT} - Daily Newspapers"

          # Collect parts into an array
          mapfile -t PARTS < <(find output -maxdepth 1 -type f -name "${BASE} - Part *.pdf" -printf "%f\n" | sort)

          if [ "${#PARTS[@]}" -gt 0 ]; then
            echo "Packing ${#PARTS[@]} merged PDF(s):"
            printf '%s\n' "${PARTS[@]}"
            tar -czf daily_news.tar.gz -C output "${PARTS[@]}"
          else
            echo "::warning::No merged PDFs found to package."
          fi

          # Logs
          if [ -d logs ]; then
            tar -czf log_daily_news.tar.gz -C logs .
          fi

          ls -lh *.tar.gz || true

      - name: Encrypt packages
        if: always()
        env:
          ARTIFACT_PASS: ${{ secrets.ARTIFACT_PASS }}
        run: |
          set -euo pipefail
          if [ -f daily_news.tar.gz ]; then
            openssl enc -aes-256-cbc -salt -pbkdf2 \
              -in daily_news.tar.gz -out daily_news.tar.gz.enc \
              -pass env:ARTIFACT_PASS
          fi
          if [ -f log_daily_news.tar.gz ]; then
            openssl enc -aes-256-cbc -salt -pbkdf2 \
              -in log_daily_news.tar.gz -out log_daily_news.tar.gz.enc \
              -pass env:ARTIFACT_PASS
          fi
          ls -lh *.enc || true

      - name: Upload encrypted artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: daily-news-encrypted-${{ github.run_id }}
          path: |
            daily_news.tar.gz.enc
            log_daily_news.tar.gz.enc
          retention-days: 14
          if-no-files-found: warn
